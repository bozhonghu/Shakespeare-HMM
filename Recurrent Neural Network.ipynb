{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets = \"\"\n",
    "with open(\"data/shakespeare.txt\") as f:\n",
    "    for line in f:\n",
    "        if (line.strip().isdigit()):\n",
    "            continue\n",
    "        else:\n",
    "            sonnets += line.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate encoding of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_dict_and_reverse(word_list):\n",
    "    \"\"\"\n",
    "    Takes a string, returning a dictionary mapping of characters to their index in a \n",
    "    one-hot-encoded representation of the words.\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    i = 0\n",
    "    for word in word_list:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = i\n",
    "            index_to_word[i] = word\n",
    "            i += 1\n",
    "    return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse into 40 character sequences and the characeter after to make the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = [], []\n",
    "for i in range(len(sonnets) - 41):\n",
    "    train_x.append(sonnets[i:i+40])\n",
    "    train_y.append(sonnets[i+40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the x vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic, index_dic = generate_onehot_dict_and_reverse(sonnets)\n",
    "train_x = [[dic[j] for j in i] for i in train_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the y labels\n",
    "train_y = [dic[i] for i in train_y]\n",
    "train_y = keras.utils.np_utils.to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at shape to confirm everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x).reshape(len(train_x), 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y).reshape(len(train_y), 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94554, 40, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94554, 38)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 40, 150)           91200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 40, 150)           0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 150)               180600    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                5738      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 277,538\n",
      "Trainable params: 277,538\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape = (40, 1, ), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(38))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "94554/94554 [==============================] - 193s 2ms/step - loss: 2.8442 - acc: 0.1941\n",
      "Epoch 2/20\n",
      "94554/94554 [==============================] - 192s 2ms/step - loss: 2.5255 - acc: 0.2856\n",
      "Epoch 3/20\n",
      "94554/94554 [==============================] - 193s 2ms/step - loss: 2.3213 - acc: 0.3291\n",
      "Epoch 4/20\n",
      "94554/94554 [==============================] - 193s 2ms/step - loss: 2.2190 - acc: 0.3562\n",
      "Epoch 5/20\n",
      "94554/94554 [==============================] - 193s 2ms/step - loss: 2.1480 - acc: 0.3735\n",
      "Epoch 6/20\n",
      "94554/94554 [==============================] - 193s 2ms/step - loss: 2.0908 - acc: 0.3878\n",
      "Epoch 7/20\n",
      "26000/94554 [=======>......................] - ETA: 2:22 - loss: 2.0488 - acc: 0.4012"
     ]
    }
   ],
   "source": [
    "## In the line below we have specified the loss function as 'mse' (Mean Squared Error) because in the above code we did not one-hot encode the labels.\n",
    "## In your implementation, since you are one-hot encoding the labels, you should use 'categorical_crossentropy' as your loss.\n",
    "## You will likely have the best results with RMS prop or Adam as your optimizer.  In the line below we use Adadelta\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(train_x, train_y, batch_size=200, nb_epoch=20,\n",
    "    verbose=1)\n",
    "\n",
    "## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "score = model.evaluate(train_x, train_y, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\n",
    "for i in train_x[0]:\n",
    "    s += index_dic[i[0]]\n",
    "seed = train_x[0]\n",
    "\n",
    "break_counter = 0\n",
    "while(break_counter < 14):\n",
    "    p = model.predict_proba(np.array(seed).reshape(1, 40, 1), verbose=0)[0]\n",
    "    n = np.random.choice(range(38), p = p)\n",
    "    c = index_dic[n]\n",
    "    s += c\n",
    "    if (n == dic[\"\\n\"]):\n",
    "        break_counter += 1\n",
    "    seed = np.append(seed[1:], [n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
