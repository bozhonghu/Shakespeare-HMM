{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnets = \"\"\n",
    "with open(\"data/shakespeare.txt\") as f:\n",
    "    for line in f:\n",
    "        if (line.strip().isdigit()):\n",
    "            continue\n",
    "        else:\n",
    "            sonnets += line.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate encoding of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_onehot_dict_and_reverse(word_list):\n",
    "    \"\"\"\n",
    "    Takes a string, returning a dictionary mapping of characters to their index in a \n",
    "    one-hot-encoded representation of the words.\n",
    "    \"\"\"\n",
    "    word_to_index = {}\n",
    "    index_to_word = {}\n",
    "    i = 0\n",
    "    for word in word_list:\n",
    "        if word not in word_to_index:\n",
    "            word_to_index[word] = i\n",
    "            index_to_word[i] = word\n",
    "            i += 1\n",
    "    return word_to_index, index_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse into 40 character sequences and the characeter after to make the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = [], []\n",
    "for i in range(len(sonnets) - 41):\n",
    "    train_x.append(sonnets[i:i+40])\n",
    "    train_y.append(sonnets[i+40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the x vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic, index_dic = generate_onehot_dict_and_reverse(sonnets)\n",
    "train_x = [[dic[j] for j in i] for i in train_x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the y labels\n",
    "train_y = [dic[i] for i in train_y]\n",
    "train_y = keras.utils.np_utils.to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at shape to confirm everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x).reshape(len(train_x), 40, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(train_y).reshape(len(train_y), 38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94554, 40, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94554, 38)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 150)               91200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                5738      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 38)                0         \n",
      "=================================================================\n",
      "Total params: 96,938\n",
      "Trainable params: 96,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, input_shape = (40, 1, )))\n",
    "model.add(Dense(38))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "## Printing a summary of the layers and weights in your model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/keras/models.py:874: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "94554/94554 [==============================] - 65s 689us/step - loss: 2.8613 - acc: 0.1902\n",
      "Epoch 2/40\n",
      "94554/94554 [==============================] - 65s 688us/step - loss: 2.6655 - acc: 0.2599\n",
      "Epoch 3/40\n",
      "94554/94554 [==============================] - 65s 690us/step - loss: 2.4772 - acc: 0.3029\n",
      "Epoch 4/40\n",
      "94554/94554 [==============================] - 65s 691us/step - loss: 2.3598 - acc: 0.3261\n",
      "Epoch 5/40\n",
      "94554/94554 [==============================] - 66s 695us/step - loss: 2.2828 - acc: 0.3425\n",
      "Epoch 6/40\n",
      "94554/94554 [==============================] - 66s 695us/step - loss: 2.2257 - acc: 0.3552\n",
      "Epoch 7/40\n",
      "94554/94554 [==============================] - 66s 699us/step - loss: 2.1822 - acc: 0.3670\n",
      "Epoch 8/40\n",
      "94554/94554 [==============================] - 66s 697us/step - loss: 2.1434 - acc: 0.3758\n",
      "Epoch 9/40\n",
      "94554/94554 [==============================] - 66s 703us/step - loss: 2.1116 - acc: 0.3857\n",
      "Epoch 10/40\n",
      "94554/94554 [==============================] - 66s 703us/step - loss: 2.0835 - acc: 0.3925\n",
      "Epoch 11/40\n",
      "94554/94554 [==============================] - 66s 699us/step - loss: 2.0588 - acc: 0.3990\n",
      "Epoch 12/40\n",
      "94554/94554 [==============================] - 68s 723us/step - loss: 2.0336 - acc: 0.4059\n",
      "Epoch 13/40\n",
      "94554/94554 [==============================] - 67s 708us/step - loss: 2.0147 - acc: 0.4105\n",
      "Epoch 14/40\n",
      "94554/94554 [==============================] - 68s 715us/step - loss: 1.9943 - acc: 0.4142\n",
      "Epoch 15/40\n",
      "94554/94554 [==============================] - 69s 725us/step - loss: 1.9760 - acc: 0.4199\n",
      "Epoch 16/40\n",
      "94554/94554 [==============================] - 69s 730us/step - loss: 1.9594 - acc: 0.4244\n",
      "Epoch 17/40\n",
      "94554/94554 [==============================] - 68s 719us/step - loss: 1.9432 - acc: 0.4273\n",
      "Epoch 18/40\n",
      "94554/94554 [==============================] - 69s 730us/step - loss: 1.9284 - acc: 0.4314\n",
      "Epoch 19/40\n",
      "94554/94554 [==============================] - 69s 734us/step - loss: 1.9151 - acc: 0.4349\n",
      "Epoch 20/40\n",
      "94554/94554 [==============================] - 70s 736us/step - loss: 1.9021 - acc: 0.4391\n",
      "Epoch 21/40\n",
      "94554/94554 [==============================] - 71s 749us/step - loss: 1.8877 - acc: 0.4426\n",
      "Epoch 22/40\n",
      "94554/94554 [==============================] - 70s 738us/step - loss: 1.8761 - acc: 0.4455\n",
      "Epoch 23/40\n",
      "94554/94554 [==============================] - 69s 727us/step - loss: 1.8642 - acc: 0.4487\n",
      "Epoch 24/40\n",
      "94554/94554 [==============================] - 69s 725us/step - loss: 1.8524 - acc: 0.4517\n",
      "Epoch 25/40\n",
      "94554/94554 [==============================] - 69s 733us/step - loss: 1.8428 - acc: 0.4557\n",
      "Epoch 26/40\n",
      "94554/94554 [==============================] - 69s 734us/step - loss: 1.8309 - acc: 0.4574\n",
      "Epoch 27/40\n",
      "94554/94554 [==============================] - 70s 741us/step - loss: 1.8212 - acc: 0.4609\n",
      "Epoch 28/40\n",
      "94554/94554 [==============================] - 69s 734us/step - loss: 1.8117 - acc: 0.4620\n",
      "Epoch 29/40\n",
      "94554/94554 [==============================] - 71s 750us/step - loss: 1.8022 - acc: 0.4646\n",
      "Epoch 30/40\n",
      "94554/94554 [==============================] - 70s 739us/step - loss: 1.7921 - acc: 0.4696\n",
      "Epoch 31/40\n",
      "94554/94554 [==============================] - 70s 743us/step - loss: 1.7838 - acc: 0.4695\n",
      "Epoch 32/40\n",
      "94554/94554 [==============================] - 71s 747us/step - loss: 1.7748 - acc: 0.4723\n",
      "Epoch 33/40\n",
      "94554/94554 [==============================] - 70s 745us/step - loss: 1.7661 - acc: 0.4750\n",
      "Epoch 34/40\n",
      "94554/94554 [==============================] - 70s 745us/step - loss: 1.7579 - acc: 0.4775\n",
      "Epoch 35/40\n",
      "94554/94554 [==============================] - 70s 745us/step - loss: 1.7485 - acc: 0.4800\n",
      "Epoch 36/40\n",
      "94554/94554 [==============================] - 71s 750us/step - loss: 1.7420 - acc: 0.4803\n",
      "Epoch 37/40\n",
      "94554/94554 [==============================] - 72s 757us/step - loss: 1.7336 - acc: 0.4826\n",
      "Epoch 38/40\n",
      "94554/94554 [==============================] - 72s 763us/step - loss: 1.7252 - acc: 0.4853\n",
      "Epoch 39/40\n",
      "94554/94554 [==============================] - 72s 760us/step - loss: 1.7195 - acc: 0.4867\n",
      "Epoch 40/40\n",
      "94554/94554 [==============================] - 71s 756us/step - loss: 1.7132 - acc: 0.4889\n",
      "Test score: 1.68429250594\n",
      "Test accuracy: 0.49663684244\n"
     ]
    }
   ],
   "source": [
    "## In the line below we have specified the loss function as 'mse' (Mean Squared Error) because in the above code we did not one-hot encode the labels.\n",
    "## In your implementation, since you are one-hot encoding the labels, you should use 'categorical_crossentropy' as your loss.\n",
    "## You will likely have the best results with RMS prop or Adam as your optimizer.  In the line below we use Adadelta\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam', metrics=['accuracy'])\n",
    "\n",
    "fit = model.fit(train_x, train_y, batch_size=200, nb_epoch=40,\n",
    "    verbose=1)\n",
    "\n",
    "## Printing the accuracy of our model, according to the loss function specified in model.compile above\n",
    "score = model.evaluate(train_x, train_y, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\n",
    "for i in train_x[0]:\n",
    "    s += index_dic[i[0]]\n",
    "seed = train_x[0]\n",
    "\n",
    "break_counter = 0\n",
    "while(break_counter < 14):\n",
    "    p = model.predict_proba(np.array(seed).reshape(1, 40, 1), verbose=0)[0]\n",
    "    n = np.random.choice(range(38), p = p)\n",
    "    c = index_dic[n]\n",
    "    s += c\n",
    "    if (n == dic[\"\\n\"]):\n",
    "        break_counter += 1\n",
    "    seed = np.append(seed[1:], [n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from fairest creatures we desire increasid,\n",
      "fouraie jikdinatous seem thy it sor lros,\n",
      "that is the pittel the touth uorws forsyt.\n",
      "\n",
      "\n",
      "what sponsane she wisasp uilt me a bove, worn the wintered,\n",
      "in lnilfua stay that wan bealen goitt.\n",
      "a are to somw tefory wimeso hreet mmin.\n",
      "m bow loeeds hut oake thous owagle sweet be,\n",
      "   f in kowe d r chat fictesed fartt,\n",
      "and do bou and meisew far i how as gous:\n",
      "lesiexy woofoot, retous-yabe be flu tpey me now wase,\n",
      " nd whills iyes faarer zike eyet mivht\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
